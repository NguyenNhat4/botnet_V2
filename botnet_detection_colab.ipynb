{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install requirements\n",
        "!pip install torchinfo>=1.8.0 numpy>=1.24.0 pandas>=2.0.0 scikit-learn>=1.4.0,<1.5.0 imbalanced-learn>=0.12.0 torch torchvision matplotlib>=3.7.0 seaborn>=0.12.0 tqdm>=4.65.0 psutil>=5.9.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CONTENT FROM: config.py\n",
        "# ==========================================\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# =============================================================================\n",
        "# SET WORKING DIRECTORY\n",
        "# =============================================================================\n",
        "WORKING_DIR = '.' \n",
        "os.makedirs(WORKING_DIR, exist_ok=True)\n",
        "\n",
        "# =============================================================================\n",
        "# SYSTEM RESOURCES & DEVICE\n",
        "# =============================================================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "BATCH_SIZE = 256\n",
        "N_WORKERS = 8 # DataLoader Workers\n",
        "\n",
        "# Check RAM (optional logic can be here or main)\n",
        "import psutil\n",
        "ram_gb = psutil.virtual_memory().total / 1e9\n",
        "\n",
        "# Training config\n",
        "N_EPOCHS = 6\n",
        "LEARNING_RATE = 0.0001\n",
        "IMAGE_SIZE = 32\n",
        "\n",
        "# =============================================================================\n",
        "# DEFINING SCENARIOS\n",
        "# =============================================================================\n",
        "# Train scenarios: \n",
        "TRAIN_SCENARIOS = ['11']\n",
        "\n",
        "# Test scenario: Rbot\n",
        "TEST_SCENARIOS = ['10']\n",
        "\n",
        "# =============================================================================\n",
        "# LABEL MAPPING\n",
        "# =============================================================================\n",
        "CLASS_TO_IDX = {\n",
        "    'Botnet': 0,\n",
        "    'C&C': 1,\n",
        "    'Normal': 2\n",
        "}\n",
        "\n",
        "# Inverse mapping\n",
        "IDX_TO_CLASS = {v: k for k, v in CLASS_TO_IDX.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CONTENT FROM: utils.py\n",
        "# ==========================================\n",
        "import os\n",
        "import ssl\n",
        "import urllib.request\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Bypass SSL verification\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "def create_directory(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    print(f\"  Created/Checked: {path}\")\n",
        "\n",
        "def download_file(url, destination):\n",
        "    try:\n",
        "        if os.path.exists(destination):\n",
        "            print(f\"  [SKIP] File exists: {os.path.basename(destination)}\")\n",
        "            return True\n",
        "\n",
        "        print(f\"  Downloading: {os.path.basename(destination)}\")\n",
        "        with tqdm(unit='B', unit_scale=True, unit_divisor=1024, miniters=1, desc=\"  Progress\") as t:\n",
        "            def reporthook(blocknum, blocksize, totalsize):\n",
        "                t.total = totalsize\n",
        "                t.update(blocknum * blocksize - t.n)\n",
        "            urllib.request.urlretrieve(url, destination, reporthook=reporthook)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {url}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def check_csv_in_folder(folder_path):\n",
        "    if not os.path.exists(folder_path): return False\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith('.csv'): return True\n",
        "    return False\n",
        "\n",
        "def rename(path_file, new_name):\n",
        "    dir_path = os.path.dirname(path_file)\n",
        "    path_new_name = os.path.join(dir_path, new_name)\n",
        "    os.rename(path_file, path_new_name)\n",
        "\n",
        "def get_csv_paths(main_dir, scenario_ids):\n",
        "    csv_paths = []\n",
        "    for sid in scenario_ids:\n",
        "        path = os.path.join(main_dir, sid)\n",
        "        # Find csv file in folder\n",
        "        if os.path.exists(path):\n",
        "            for file in os.listdir(path):\n",
        "                if file.endswith('.csv'):\n",
        "                    csv_paths.append(os.path.join(path, file))\n",
        "                    break\n",
        "    return csv_paths\n",
        "\n",
        "def plot_and_save_loss(train_losses, valid_losses, save_path):\n",
        "    \"\"\"\n",
        "    Plots Loss chart and saves to file.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot Train Loss\n",
        "    plt.plot(train_losses, label='Train Loss', color='blue', marker='o', markersize=4)\n",
        "\n",
        "    # Plot Valid Loss\n",
        "    plt.plot(valid_losses, label='Valid Loss', color='orange', marker='o', markersize=4)\n",
        "\n",
        "    # Decorate\n",
        "    plt.title('Training vs Validation Loss', fontsize=16)\n",
        "    plt.xlabel('Epochs', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Save image\n",
        "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"\\n[INFO] Saved Loss plot at: {save_path}\")\n",
        "\n",
        "    # Show (optional in script mode, but harmless)\n",
        "    # plt.show() \n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CONTENT FROM: preprocessing_utils.py\n",
        "# ==========================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def quick_classify(label):\n",
        "    \"\"\"\n",
        "    Classifies a label string into 'Botnet', 'C&C', or 'Normal'.\n",
        "    \"\"\"\n",
        "    if not isinstance(label, str):\n",
        "        return 'Normal'\n",
        "    label_lower = label.lower()\n",
        "    if 'c&c' in label_lower or 'cc' in label_lower:\n",
        "        return 'C&C'\n",
        "    elif 'botnet' in label_lower:\n",
        "        return 'Botnet'\n",
        "    else:\n",
        "        return 'Normal'\n",
        "\n",
        "def calculate_global_frequencies(csv_paths):\n",
        "    \"\"\"\n",
        "    Calculates global frequencies for IPs and Ports across all CSV files.\n",
        "    Returns a dictionary of dictionaries.\n",
        "    \"\"\"\n",
        "    freqs = {\n",
        "        'SrcAddr': {},\n",
        "        'DstAddr': {},\n",
        "        'Sport': {},\n",
        "        'Dport': {}\n",
        "    }\n",
        "    \n",
        "    print(\"Scanning CSVs for frequencies...\")\n",
        "    for path in tqdm(csv_paths, desc=\"Global Freqs\"):\n",
        "        try:\n",
        "            # Read specific columns to save memory\n",
        "            # Note: low_memory=False helps with mixed types\n",
        "            df = pd.read_csv(path, usecols=['SrcAddr', 'DstAddr', 'Sport', 'Dport'], low_memory=False)\n",
        "            \n",
        "            for col in freqs:\n",
        "                if col in df.columns:\n",
        "                    # Value counts\n",
        "                    vc = df[col].value_counts().to_dict()\n",
        "                    for k, v in vc.items():\n",
        "                        # We use simple types for keys to ensure matches later\n",
        "                        # Convert key to string if needed?\n",
        "                        # Ports might be int or str. IPs are str.\n",
        "                        # Let's keep original types but be careful in mapping.\n",
        "                        # Actually, better to normalize to string for consistency?\n",
        "                        # But process_batch converts to numeric for ports.\n",
        "                        # Let's stick to raw values here, assume map handles it or we align later.\n",
        "                        # But for safety, maybe convert to string?\n",
        "                        # The notebook output says \"Unique Source Ports: 61941\".\n",
        "                        \n",
        "                        freqs[col][k] = freqs[col].get(k, 0) + v\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not process {path} for frequencies: {e}\")\n",
        "            \n",
        "    return freqs\n",
        "\n",
        "def process_batch_fast_v2(chunk, freq_dicts, expected_columns=None):\n",
        "    \"\"\"\n",
        "    Processes a batch (DataFrame chunk).\n",
        "    1. Cleans data.\n",
        "    2. Encodes features (Frequencies, One-Hot).\n",
        "    3. Aligns columns.\n",
        "    Returns (X_values, y_values, columns_list).\n",
        "    \"\"\"\n",
        "    df = chunk.copy()\n",
        "    \n",
        "    # 1. Label Processing\n",
        "    if 'Label' in df.columns:\n",
        "        y = df['Label'].apply(quick_classify)\n",
        "        df = df.drop(columns=['Label'])\n",
        "    else:\n",
        "        y = None \n",
        "\n",
        "    # 2. Basic Cleaning\n",
        "    if 'StartTime' in df.columns:\n",
        "        df = df.drop(columns=['StartTime'])\n",
        "        \n",
        "    # 3. Numeric Conversions\n",
        "    # Columns that should be numeric\n",
        "    numeric_cols = ['Dur', 'TotPkts', 'TotBytes', 'SrcBytes']\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "            \n",
        "    # 4. Feature Engineering: Frequencies\n",
        "    for col in ['SrcAddr', 'DstAddr', 'Sport', 'Dport']:\n",
        "        freq_col_name = f\"{col}_freq\"\n",
        "        if col in df.columns and col in freq_dicts:\n",
        "            # Map values. Using map with a dict. \n",
        "            # Note: If types don't match (e.g. int vs str), map produces NaN.\n",
        "            # We assume consistent types from read_csv.\n",
        "            df[freq_col_name] = df[col].map(freq_dicts.get(col, {})).fillna(0)\n",
        "        else:\n",
        "            df[freq_col_name] = 0\n",
        "\n",
        "    # 5. Split IP th\u00e0nh 4 octet (4 feature ri\u00eang) thay v\u00ec m\u1ed9t integer l\u1edbn\n",
        "    for ip_col in ['SrcAddr', 'DstAddr']:\n",
        "        if ip_col in df.columns:\n",
        "            # Chuy\u1ec3n sang string \u0111\u1ec3 split theo '.'\n",
        "            ip_str = df[ip_col].astype(str)\n",
        "            parts = ip_str.str.split('.', expand=True)\n",
        "\n",
        "            # \u0110\u1ea3m b\u1ea3o c\u00f3 \u0111\u1ee7 4 c\u1ed9t (n\u1ebfu IP kh\u00f4ng chu\u1ea9n, ph\u1ea7n thi\u1ebfu s\u1ebd l\u00e0 NaN)\n",
        "            for i in range(4):\n",
        "                if i < parts.shape[1]:\n",
        "                    df[f'{ip_col}_octet_{i+1}'] = pd.to_numeric(parts[i], errors='coerce').fillna(0)\n",
        "                else:\n",
        "                    # N\u1ebfu thi\u1ebfu c\u1ed9t, t\u1ea1o c\u1ed9t to\u00e0n 0\n",
        "                    df[f'{ip_col}_octet_{i+1}'] = 0\n",
        "\n",
        "    # 6. Handle Port Columns (Keep as numeric features)\n",
        "    for col in ['Sport', 'Dport']:\n",
        "        if col in df.columns:\n",
        "            # Force numeric (handle hex or strings)\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "\n",
        "    # 7. Drop raw IP addresses (\u0111\u00e3 c\u00f3 t\u1ea7n su\u1ea5t + 4 octet)\n",
        "    df = df.drop(columns=['SrcAddr', 'DstAddr'], errors='ignore')\n",
        "    \n",
        "    # 8. One-Hot Encoding for 'Proto'\n",
        "    if 'Proto' in df.columns:\n",
        "        # Common protos: TCP, UDP, ICMP\n",
        "        # Limit cardinality if needed? Usually low.\n",
        "        dummies = pd.get_dummies(df['Proto'], prefix='Proto')\n",
        "        df = pd.concat([df, dummies], axis=1)\n",
        "        df = df.drop(columns=['Proto'])\n",
        "        \n",
        "    # 8. One-Hot Encoding for 'State' (kh\u00f4ng gi\u1edbi h\u1ea1n top state)\n",
        "    if 'State' in df.columns:\n",
        "        dummies = pd.get_dummies(df['State'], prefix='State')\n",
        "        df = pd.concat([df, dummies], axis=1)\n",
        "        df = df.drop(columns=['State'])\n",
        "        \n",
        "    # 9. Handle 'Dir'\n",
        "    if 'Dir' in df.columns:\n",
        "         dummies = pd.get_dummies(df['Dir'], prefix='Dir')\n",
        "         df = pd.concat([df, dummies], axis=1)\n",
        "         df = df.drop(columns=['Dir'])\n",
        "         \n",
        "    # 10. Align columns with expected_columns\n",
        "    if expected_columns is not None:\n",
        "        # Add missing columns with 0\n",
        "        # Use reindex for efficiency and handling both missing and extra\n",
        "        df = df.reindex(columns=expected_columns, fill_value=0)\n",
        "    else:\n",
        "        # First time detection: Fill NaNs just in case\n",
        "        df = df.fillna(0)\n",
        "        \n",
        "    # Return values\n",
        "    X_vals = df.values.astype(np.float32) # Ensure float32 for PyTorch\n",
        "    y_vals = y.values if y is not None else None\n",
        "    \n",
        "    return X_vals, y_vals, df.columns.tolist()\n",
        "\n",
        "def save_global_stats(global_stats, filepath='global_stats.pkl'):\n",
        "    \"\"\"Saves global statistics to a pickle file.\"\"\"\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(global_stats, f)\n",
        "    print(f\"Global stats saved to {filepath}\")\n",
        "\n",
        "def load_global_stats(filepath='global_stats.pkl'):\n",
        "    \"\"\"Loads global statistics from a pickle file.\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"{filepath} not found.\")\n",
        "    with open(filepath, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def save_scaler(scaler, filepath='scaler.pkl'):\n",
        "    \"\"\"Saves the scaler to a pickle file.\"\"\"\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "    print(f\"Scaler saved to {filepath}\")\n",
        "\n",
        "def load_scaler(filepath='scaler.pkl'):\n",
        "    \"\"\"Loads the scaler from a pickle file.\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"{filepath} not found.\")\n",
        "    with open(filepath, 'rb') as f:\n",
        "        return pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CONTENT FROM: loss.py\n",
        "# ==========================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, weight=None, gamma=2., reduction='mean'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            weight (Tensor, optional): A manual rescaling weight given to each class.\n",
        "                                       If given, has to be a Tensor of size C.\n",
        "            gamma (float): Focusing parameter.\n",
        "            reduction (string, optional): Specifies the reduction to apply to the output:\n",
        "                                          'none' | 'mean' | 'sum'. 'mean': the sum of the output will be divided by the number of elements in the output,\n",
        "                                          'sum': the output will be summed. Default: 'mean'\n",
        "        \"\"\"\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.weight = weight\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: (N, C) where C = number of classes.\n",
        "            targets: (N) where each value is 0 <= targets[i] <= C-1.\n",
        "        \"\"\"\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.weight)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CONTENT FROM: model.py\n",
        "# ==========================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "class BotnetImageCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN 2D d\u00f9ng pretrained MobileNetV3-Small.\n",
        "    - Input: \u1ea3nh 1x32x32\n",
        "    - N\u1ed9i b\u1ed9 resize l\u00ean 224x224 cho ph\u00f9 h\u1ee3p v\u1edbi pretrained weights.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_classes: int = 3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Pretrained MobileNetV3 Small (nh\u1eb9)\n",
        "        weights = models.MobileNet_V3_Small_Weights.DEFAULT\n",
        "        base = models.mobilenet_v3_small(weights=weights)\n",
        "\n",
        "        # Ch\u1ec9nh conv \u0111\u1ea7u ti\u00ean nh\u1eadn 1 k\u00eanh thay v\u00ec 3 k\u00eanh\n",
        "        first_conv = base.features[0][0]\n",
        "        new_conv = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=first_conv.out_channels,\n",
        "            kernel_size=first_conv.kernel_size,\n",
        "            stride=first_conv.stride,\n",
        "            padding=first_conv.padding,\n",
        "            bias=False,\n",
        "        )\n",
        "\n",
        "        # Kh\u1edfi t\u1ea1o tr\u1ecdng s\u1ed1 t\u1eeb pretrained: trung b\u00ecnh theo channel\n",
        "        with torch.no_grad():\n",
        "            new_conv.weight[:] = first_conv.weight.mean(dim=1, keepdim=True)\n",
        "\n",
        "        base.features[0][0] = new_conv\n",
        "\n",
        "        # Thay classifier cu\u1ed1i cho \u0111\u00fang s\u1ed1 l\u1edbp\n",
        "        in_features = base.classifier[-1].in_features\n",
        "        base.classifier[-1] = nn.Linear(in_features, n_classes)\n",
        "\n",
        "        self.base = base\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: [B, 1, 32, 32] -> resize 224x224 cho MobileNet\n",
        "        x = F.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
        "        return self.base(x)\n",
        "\n",
        "\n",
        "class BotnetClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper gi\u1eef API c\u0169:\n",
        "      - v\u1eabn nh\u1eadn n_features, image_size nh\u01b0ng b\u1ecf qua; d\u1eef li\u1ec7u \u0111\u00e3 l\u00e0 \u1ea3nh 1x32x32.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_model=None, n_features=None, image_size: int = 32, n_classes: int = 3):\n",
        "        super().__init__()\n",
        "        self.model = BotnetImageCNN(n_classes=n_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CONTENT FROM: data_loader.py\n",
        "# ==========================================\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# from preprocessing_utils import process_batch_fast_v2  # Commented out for notebook compatibility\n",
        "# from config import CLASS_TO_IDX, IMAGE_SIZE  # Commented out for notebook compatibility\n",
        "\n",
        "\n",
        "class FastBotnetDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset:\n",
        "      - Nh\u1eadn X_data d\u1ea1ng [N, n_features]\n",
        "      - M\u1ed7i d\u00f2ng \u0111\u01b0\u1ee3c pad / c\u1eaft v\u1ec1 32x32 v\u00e0 reshape th\u00e0nh \u1ea3nh 1x32x32\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = torch.from_numpy(X_data).float()\n",
        "        self.y_data = torch.from_numpy(y_data).long()\n",
        "\n",
        "        self.image_size = IMAGE_SIZE\n",
        "        self.num_pixels = self.image_size * self.image_size\n",
        "\n",
        "        # Pre-compute Hilbert curve coordinates for mapping 1D features -> 2D \u1ea3nh\n",
        "        self.hilbert_coords = self._hilbert_curve(self.image_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_data)\n",
        "\n",
        "    def _hilbert_curve(self, n):\n",
        "        \"\"\"\n",
        "        Sinh danh s\u00e1ch t\u1ecda \u0111\u1ed9 (row, col) theo \u0111\u01b0\u1eddng cong Hilbert cho l\u01b0\u1edbi n x n.\n",
        "        n ph\u1ea3i l\u00e0 l\u0169y th\u1eeba c\u1ee7a 2 (\u1edf \u0111\u00e2y n = 32).\n",
        "        Thu\u1eadt to\u00e1n \u0111\u01a1n gi\u1ea3n, \u01b0u ti\u00ean t\u00ednh d\u1ec5 \u0111\u1ecdc h\u01a1n l\u00e0 t\u1ed1i \u01b0u tuy\u1ec7t \u0111\u1ed1i.\n",
        "        \"\"\"\n",
        "        if n & (n - 1) != 0:\n",
        "            raise ValueError(\"IMAGE_SIZE must be a power of 2 for Hilbert curve.\")\n",
        "\n",
        "        def hilbert_index_to_xy(d, order):\n",
        "            \"\"\"\n",
        "            Chuy\u1ec3n ch\u1ec9 s\u1ed1 1D d tr\u00ean Hilbert order 'order' (2^order x 2^order)\n",
        "            sang t\u1ecda \u0111\u1ed9 (x, y).\n",
        "            \"\"\"\n",
        "            x = y = 0\n",
        "            t = d\n",
        "            s = 1\n",
        "            while s < (1 << order):\n",
        "                rx = 1 & (t // 2)\n",
        "                ry = 1 & (t ^ rx)\n",
        "                # xoay / ph\u1ea3n chi\u1ebfu\n",
        "                if ry == 0:\n",
        "                    if rx == 1:\n",
        "                        x, y = s - 1 - x, s - 1 - y\n",
        "                    x, y = y, x\n",
        "                x += s * rx\n",
        "                y += s * ry\n",
        "                t //= 4\n",
        "                s <<= 1\n",
        "            return x, y\n",
        "\n",
        "        order = int(np.log2(n))\n",
        "        coords = []\n",
        "        for d in range(n * n):\n",
        "            x, y = hilbert_index_to_xy(d, order)\n",
        "            coords.append((x, y))\n",
        "        return coords\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # vector \u0111\u1eb7c tr\u01b0ng 1D: [n_features]\n",
        "        feature_vector = self.X_data[idx]\n",
        "\n",
        "        # Kh\u1edfi t\u1ea1o \u1ea3nh 0 (0..255), s\u1ebd \u0111i\u1ec1n theo \u0111\u01b0\u1eddng cong Hilbert\n",
        "        image = torch.zeros((1, self.image_size, self.image_size), dtype=feature_vector.dtype)\n",
        "\n",
        "        # S\u1ed1 feature th\u1ef1c s\u1ef1 c\u00f3 (kh\u00f4ng b\u1ecf d\u00f2ng n\u00e0o, ch\u1ec9 gi\u1edbi h\u1ea1n s\u1ed1 pixel)\n",
        "        length = min(feature_vector.numel(), self.num_pixels)\n",
        "\n",
        "        # Map t\u1eebng feature l\u00ean pixel theo Hilbert curve \u0111\u1ec3 gi\u1eef locality\n",
        "        for i in range(length):\n",
        "            x, y = self.hilbert_coords[i]\n",
        "            image[0, x, y] = feature_vector[i]\n",
        "\n",
        "        # Scale per-sample v\u1ec1 kho\u1ea3ng [0, 255]\n",
        "        vals = image.view(-1)\n",
        "        vals = torch.clamp(vals, min=0)\n",
        "        min_val = vals.min()\n",
        "        max_val = vals.max()\n",
        "        if max_val > min_val:\n",
        "            vals = (vals - min_val) / (max_val - min_val)\n",
        "        else:\n",
        "            vals = vals * 0.0\n",
        "        vals = vals * 255.0\n",
        "        image = vals.view(1, self.image_size, self.image_size)\n",
        "\n",
        "        return image, self.y_data[idx]\n",
        "\n",
        "def load_data_from_csvs(csv_list, global_stats, desc=\"Loading\", is_train=True, scaler=None):\n",
        "    \"\"\"\n",
        "    Loads data from a list of CSVs, processes it, and returns X and y.\n",
        "    \"\"\"\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    # Get stats\n",
        "    freq_dicts = global_stats['freq_dicts']\n",
        "    expected_cols = global_stats['expected_columns']\n",
        "\n",
        "    for csv_file in tqdm(csv_list, desc=desc):\n",
        "        try:\n",
        "            for chunk in pd.read_csv(csv_file, chunksize=100000, low_memory=False):\n",
        "                X_batch, y_batch, _ = process_batch_fast_v2(\n",
        "                    chunk, freq_dicts, expected_cols\n",
        "                )\n",
        "                if len(X_batch) > 0:\n",
        "                    X_list.append(X_batch)\n",
        "                    # Convert labels to indices\n",
        "                    # y_batch is a Series or numpy array of strings\n",
        "                    if y_batch is not None:\n",
        "                         # Ensure y_batch contains valid keys from CLASS_TO_IDX\n",
        "                        y_indices = np.array([CLASS_TO_IDX.get(label, 2) for label in y_batch]) # Default to Normal (2) if not found\n",
        "                        y_list.append(y_indices)\n",
        "        except Exception as e:\n",
        "            print(f\"  Error loading {csv_file}: {e}\")\n",
        "\n",
        "    if not X_list:\n",
        "        return np.array([]), np.array([])\n",
        "\n",
        "    X_data = np.vstack(X_list).astype(np.float32)\n",
        "    y_data = np.concatenate(y_list).astype(np.int64)\n",
        "    # 1. Sanitize input: Replace existing NaNs/Infs with 0\n",
        "    X_data = np.nan_to_num(X_data, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    # 2. Clip negative values to 0. \n",
        "    # This prevents np.log1p(-1) from becoming -inf.\n",
        "    X_data = np.maximum(X_data, 0)\n",
        "\n",
        "    # Normalize (Log1p + Scaler)\n",
        "    X_data = np.log1p(X_data)\n",
        "\n",
        "    if scaler:\n",
        "        if is_train:\n",
        "            X_data = scaler.fit_transform(X_data)\n",
        "        else:\n",
        "            X_data = scaler.transform(X_data)\n",
        "\n",
        "    return X_data, y_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CONTENT FROM: analyze_csv.py\n",
        "# ==========================================\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# from preprocessing_utils import quick_classify  # Commented out for notebook compatibility\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "# Ensure matplotlib can display Vietnamese characters if supported font is found, \n",
        "# otherwise fallback to default. \n",
        "# In a standard env, this might be tricky, so we stick to standard fonts but write strings in Vietnamese.\n",
        "\n",
        "def load_data(filepath):\n",
        "    \"\"\"Loads CSV data.\"\"\"\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"Kh\u00f4ng t\u00ecm th\u1ea5y file: {filepath}\")\n",
        "    \n",
        "    # Check if empty\n",
        "    if os.path.getsize(filepath) == 0:\n",
        "         raise ValueError(f\"File r\u1ed7ng: {filepath}\")\n",
        "\n",
        "    print(f\"\u0110ang t\u1ea3i d\u1eef li\u1ec7u t\u1eeb: {filepath} ...\")\n",
        "    # Low memory false to handle mixed types if any, similar to training script\n",
        "    df = pd.read_csv(filepath, low_memory=False)\n",
        "    return df\n",
        "\n",
        "def analyze_data(df):\n",
        "    \"\"\"Calculates statistics and metrics.\"\"\"\n",
        "    stats = {}\n",
        "    \n",
        "    # 1. Label Mapping\n",
        "    if 'Label' in df.columns:\n",
        "        df['Mapped_Label'] = df['Label'].apply(quick_classify)\n",
        "        \n",
        "        # Raw Label Counts\n",
        "        stats['raw_label_counts'] = df['Label'].value_counts().head(20).to_dict()\n",
        "        \n",
        "        # Mapped Label Counts\n",
        "        stats['mapped_label_counts'] = df['Mapped_Label'].value_counts().to_dict()\n",
        "        stats['mapped_label_pct'] = df['Mapped_Label'].value_counts(normalize=True).to_dict()\n",
        "    else:\n",
        "        stats['raw_label_counts'] = {}\n",
        "        stats['mapped_label_counts'] = {}\n",
        "        print(\"C\u1ea2NH B\u00c1O: Kh\u00f4ng t\u00ecm th\u1ea5y c\u1ed9t 'Label'.\")\n",
        "\n",
        "    # 2. Numerical Features Stats\n",
        "    num_cols = ['Dur', 'TotPkts', 'TotBytes', 'SrcBytes']\n",
        "    stats['numerical'] = {}\n",
        "    \n",
        "    for col in num_cols:\n",
        "        if col in df.columns:\n",
        "            # Force numeric\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            \n",
        "            desc = df[col].describe()\n",
        "            stats['numerical'][col] = {\n",
        "                'mean': desc['mean'],\n",
        "                'std': desc['std'],\n",
        "                'min': desc['min'],\n",
        "                'max': desc['max'],\n",
        "                'median': desc['50%']\n",
        "            }\n",
        "        else:\n",
        "            stats['numerical'][col] = None\n",
        "\n",
        "    # 3. Categorical Counts\n",
        "    cat_cols = ['Proto', 'State', 'Dir']\n",
        "    stats['categorical'] = {}\n",
        "    for col in cat_cols:\n",
        "        if col in df.columns:\n",
        "            stats['categorical'][col] = df[col].value_counts().head(10).to_dict()\n",
        "        else:\n",
        "            stats['categorical'][col] = {}\n",
        "            \n",
        "    # 4. Missing Values\n",
        "    stats['missing_values'] = df.isnull().sum().to_dict()\n",
        "    \n",
        "    return stats, df\n",
        "\n",
        "def generate_report(stats, df, output_dir, filename_base):\n",
        "    \"\"\"Generates text report and plots.\"\"\"\n",
        "    \n",
        "    # --- 1. Text Report ---\n",
        "    report_path = os.path.join(output_dir, f\"{filename_base}_report.txt\")\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"B\u00c1O C\u00c1O PH\u00c2N T\u00cdCH D\u1eee LI\u1ec6U: {filename_base}\\n\")\n",
        "        f.write(\"=\"*60 + \"\\n\\n\")\n",
        "        \n",
        "        f.write(\"1. T\u1ed4NG QUAN\\n\")\n",
        "        f.write(f\"   - T\u1ed5ng s\u1ed1 d\u00f2ng: {len(df)}\\n\")\n",
        "        f.write(f\"   - S\u1ed1 c\u1ed9t: {len(df.columns)}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "        \n",
        "        f.write(\"2. PH\u00c2N B\u1ed0 NH\u00c3N (L\u1edbp Mapped)\\n\")\n",
        "        if stats['mapped_label_counts']:\n",
        "            for label, count in stats['mapped_label_counts'].items():\n",
        "                pct = stats['mapped_label_pct'].get(label, 0) * 100\n",
        "                f.write(f\"   - {label}: {count} ({pct:.2f}%)\\n\")\n",
        "        else:\n",
        "            f.write(\"   (Kh\u00f4ng c\u00f3 d\u1eef li\u1ec7u nh\u00e3n)\\n\")\n",
        "        f.write(\"\\n\")\n",
        "        \n",
        "        f.write(\"3. TOP 20 NH\u00c3N G\u1ed0C (Raw Labels)\\n\")\n",
        "        if stats['raw_label_counts']:\n",
        "            for label, count in stats['raw_label_counts'].items():\n",
        "                f.write(f\"   - {label}: {count}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "        \n",
        "        f.write(\"4. TH\u1ed0NG K\u00ca S\u1ed0 H\u1eccC (Numerical Stats)\\n\")\n",
        "        for col, data in stats['numerical'].items():\n",
        "            if data:\n",
        "                f.write(f\"   * {col}:\\n\")\n",
        "                f.write(f\"     - Trung b\u00ecnh (Mean): {data['mean']:.2f}\\n\")\n",
        "                f.write(f\"     - Trung v\u1ecb (Median): {data['median']:.2f}\\n\")\n",
        "                f.write(f\"     - L\u1edbn nh\u1ea5t (Max):    {data['max']:.2f}\\n\")\n",
        "                f.write(f\"     - \u0110\u1ed9 l\u1ec7ch chu\u1ea9n (Std): {data['std']:.2f}\\n\")\n",
        "            else:\n",
        "                f.write(f\"   * {col}: Kh\u00f4ng t\u00ecm th\u1ea5y\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"5. TH\u00d4NG TIN PH\u00c2N LO\u1ea0I (Categorical Info)\\n\")\n",
        "        for col, data in stats['categorical'].items():\n",
        "            f.write(f\"   * Top 10 {col}:\\n\")\n",
        "            for k, v in data.items():\n",
        "                f.write(f\"     - {k}: {v}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"6. GI\u00c1 TR\u1eca THI\u1ebeU (Missing Values)\\n\")\n",
        "        for col, count in stats['missing_values'].items():\n",
        "            if count > 0:\n",
        "                f.write(f\"   - {col}: {count}\\n\")\n",
        "    \n",
        "    print(f\"\u0110\u00e3 l\u01b0u b\u00e1o c\u00e1o v\u0103n b\u1ea3n t\u1ea1i: {report_path}\")\n",
        "\n",
        "    # --- 2. Visualization ---\n",
        "    image_path = os.path.join(output_dir, f\"{filename_base}_visuals.png\")\n",
        "    \n",
        "    # Setup Figure: 2 Rows, 3 Columns\n",
        "    fig = plt.figure(figsize=(20, 12), constrained_layout=True)\n",
        "    gs = fig.add_gridspec(2, 3)\n",
        "\n",
        "    # A. Class Distribution (Mapped)\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    if 'Mapped_Label' in df.columns:\n",
        "        sns.countplot(x='Mapped_Label', data=df, ax=ax1, palette='viridis', hue='Mapped_Label', legend=False)\n",
        "        ax1.set_title(\"Ph\u00e2n b\u1ed1 L\u1edbp (Mapped Classes)\", fontsize=14)\n",
        "        ax1.set_ylabel(\"S\u1ed1 l\u01b0\u1ee3ng\")\n",
        "        ax1.set_xlabel(\"L\u1edbp\")\n",
        "    else:\n",
        "        ax1.text(0.5, 0.5, \"Kh\u00f4ng c\u00f3 d\u1eef li\u1ec7u nh\u00e3n\", ha='center')\n",
        "\n",
        "    # B. Numerical Distributions (Boxplots Log Scale)\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    # Melting for easier plotting of multiple features\n",
        "    num_cols = ['Dur', 'TotPkts', 'TotBytes', 'SrcBytes']\n",
        "    # Filter valid columns\n",
        "    valid_num = [c for c in num_cols if c in df.columns]\n",
        "    \n",
        "    if valid_num:\n",
        "        # We take a sample if data is too huge to plot quickly? No, boxplot is fast enough usually.\n",
        "        # But we need log scale because Bytes can be huge.\n",
        "        df_melt = df[valid_num].melt(var_name='Feature', value_name='Value')\n",
        "        # Log transformation for display (handling 0)\n",
        "        df_melt['LogValue'] = np.log1p(df_melt['Value'])\n",
        "        \n",
        "        sns.boxplot(x='Feature', y='LogValue', data=df_melt, ax=ax2, hue='Feature', palette=\"Set2\")\n",
        "        ax2.set_title(\"Ph\u00e2n b\u1ed1 \u0110\u1eb7c tr\u01b0ng S\u1ed1 (Log Scale)\", fontsize=14)\n",
        "        ax2.set_ylabel(\"Log(Gi\u00e1 tr\u1ecb + 1)\")\n",
        "    else:\n",
        "        ax2.text(0.5, 0.5, \"Kh\u00f4ng c\u00f3 d\u1eef li\u1ec7u s\u1ed1\", ha='center')\n",
        "\n",
        "    # C. Top Raw Labels (Horizontal Bar)\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "    if 'Label' in df.columns:\n",
        "        top_labels = df['Label'].value_counts().head(10)\n",
        "        sns.barplot(y=top_labels.index, x=top_labels.values, ax=ax3, palette=\"magma\", hue=top_labels.index, legend=False)\n",
        "        ax3.set_title(\"Top 10 Nh\u00e3n G\u1ed1c (Raw Labels)\", fontsize=14)\n",
        "        ax3.set_xlabel(\"S\u1ed1 l\u01b0\u1ee3ng\")\n",
        "    else:\n",
        "        ax3.text(0.5, 0.5, \"Kh\u00f4ng c\u00f3 d\u1eef li\u1ec7u nh\u00e3n\", ha='center')\n",
        "\n",
        "    # D. Protocol Distribution\n",
        "    ax4 = fig.add_subplot(gs[1, 0])\n",
        "    if 'Proto' in df.columns:\n",
        "        top_proto = df['Proto'].value_counts().head(10)\n",
        "        sns.barplot(x=top_proto.index, y=top_proto.values, ax=ax4, palette=\"Blues_d\", hue=top_proto.index, legend=False)\n",
        "        ax4.set_title(\"Top Giao th\u1ee9c (Protocol)\", fontsize=14)\n",
        "        ax4.set_ylabel(\"S\u1ed1 l\u01b0\u1ee3ng\")\n",
        "    else:\n",
        "        ax4.text(0.5, 0.5, \"Kh\u00f4ng c\u00f3 Proto\", ha='center')\n",
        "\n",
        "    # E. State Distribution\n",
        "    ax5 = fig.add_subplot(gs[1, 1])\n",
        "    if 'State' in df.columns:\n",
        "        top_state = df['State'].value_counts().head(10)\n",
        "        sns.barplot(x=top_state.index, y=top_state.values, ax=ax5, palette=\"Reds_d\", hue=top_state.index, legend=False)\n",
        "        ax5.set_title(\"Top Tr\u1ea1ng th\u00e1i (State)\", fontsize=14)\n",
        "        ax5.set_ylabel(\"S\u1ed1 l\u01b0\u1ee3ng\")\n",
        "        ax5.tick_params(axis='x', rotation=45)\n",
        "    else:\n",
        "        ax5.text(0.5, 0.5, \"Kh\u00f4ng c\u00f3 State\", ha='center')\n",
        "\n",
        "    # F. Feature by Class (e.g., TotBytes per Class)\n",
        "    ax6 = fig.add_subplot(gs[1, 2])\n",
        "    if 'Mapped_Label' in df.columns and 'TotBytes' in df.columns:\n",
        "         # Use LogBytes\n",
        "         df['LogTotBytes'] = np.log1p(df['TotBytes'])\n",
        "         sns.boxplot(x='Mapped_Label', y='LogTotBytes', data=df, ax=ax6, palette=\"coolwarm\", hue='Mapped_Label', legend=False)\n",
        "         ax6.set_title(\"TotBytes theo L\u1edbp (Log Scale)\", fontsize=14)\n",
        "         ax6.set_ylabel(\"Log(TotBytes)\")\n",
        "    else:\n",
        "        ax6.text(0.5, 0.5, \"Thi\u1ebfu d\u1eef li\u1ec7u \u0111\u1ec3 v\u1ebd bi\u1ec3u \u0111\u1ed3\", ha='center')\n",
        "\n",
        "    # Save\n",
        "    plt.suptitle(f\"Dashboard Ph\u00e2n T\u00edch: {filename_base}\", fontsize=20)\n",
        "    plt.savefig(image_path)\n",
        "    plt.close()\n",
        "    \n",
        "    print(f\"\u0110\u00e3 l\u01b0u bi\u1ec3u \u0111\u1ed3 t\u1ea1i: {image_path}\")\n",
        "\n",
        "\n",
        "def analyze_csv_main():\n",
        "    parser = argparse.ArgumentParser(description=\"Script ph\u00e2n t\u00edch file CSV CTU-13\")\n",
        "    parser.add_argument(\"--file\", type=str, required=True, help=\"\u0110\u01b0\u1eddng d\u1eabn \u0111\u1ebfn file .csv\")\n",
        "    parser.add_argument(\"--outdir\", type=str, default=\"analysis_reports\", help=\"Th\u01b0 m\u1ee5c l\u01b0u k\u1ebft qu\u1ea3\")\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    # Create output dir\n",
        "    os.makedirs(args.outdir, exist_ok=True)\n",
        "    \n",
        "    # Filename base\n",
        "    base_name = os.path.splitext(os.path.basename(args.file))[0]\n",
        "    # To avoid overwriting if same filename exists in different folders, maybe prepend parent folder?\n",
        "    # e.g. 10_10.csv\n",
        "    parent_dir = os.path.basename(os.path.dirname(args.file))\n",
        "    if parent_dir.isdigit():\n",
        "        base_name = f\"{parent_dir}_{base_name}\"\n",
        "    \n",
        "    try:\n",
        "        df = load_data(args.file)\n",
        "        stats, df_processed = analyze_data(df)\n",
        "        generate_report(stats, df_processed, args.outdir, base_name)\n",
        "        print(\"Ho\u00e0n t\u1ea5t!\")\n",
        "    except Exception as e:\n",
        "        print(f\"L\u1ed6I: {e}\")\n",
        "\n",
        "# if __name__ == \"__main__\": # Block disabled for notebook import\n",
        "#     analyze_csv_main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CONTENT FROM: train.py\n",
        "# ==========================================\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.utils import class_weight\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tqdm.auto import tqdm\n",
        "# from loss import FocalLoss  # Commented out for notebook compatibility\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except ImportError:\n",
        "    summary = None\n",
        "\n",
        "# from config import (  # Commented out for notebook compatibility\n",
        "    WORKING_DIR, BATCH_SIZE, N_WORKERS, N_EPOCHS, LEARNING_RATE,\n",
        "    IMAGE_SIZE, TRAIN_SCENARIOS, TEST_SCENARIOS,\n",
        "    CLASS_TO_IDX, device\n",
        ")\n",
        "# from utils import (  # Commented out for notebook compatibility\n",
        "    create_directory, download_file, check_csv_in_folder, \n",
        "    rename, get_csv_paths, plot_and_save_loss\n",
        ")\n",
        "# from preprocessing_utils import (  # Commented out for notebook compatibility\n",
        "    quick_classify, calculate_global_frequencies, process_batch_fast_v2,\n",
        "    save_global_stats, save_scaler\n",
        ")\n",
        "# from model import BotnetClassifier  # Commented out for notebook compatibility\n",
        "# from data_loader import FastBotnetDataset, load_data_from_csvs  # Commented out for notebook compatibility\n",
        "\n",
        "def train_main(ispart=True):\n",
        "    print(f\"Working directory: {os.getcwd()}\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"SYSTEM RESOURCES\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    if device.type == 'cuda':\n",
        "        print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        \n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"CONFIGURATION\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "    print(f\"  Epochs: {N_EPOCHS}\")\n",
        "    print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
        "    print(f\"  DataLoader Workers: {N_WORKERS}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # =============================================================================\n",
        "    # 1. DOWNLOAD DATASET\n",
        "    # =============================================================================\n",
        "    main_dir = './CTU-13-Dataset/'\n",
        "    create_directory(main_dir)\n",
        "\n",
        "    for i in range(1, 14):\n",
        "        create_directory(os.path.join(main_dir, str(i)))\n",
        "\n",
        "    datasets = [\n",
        "        (1, 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-42/detailed-bidirectional-flow-labels/capture20110810.binetflow'),\n",
        "        (2, 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-43/detailed-bidirectional-flow-labels/capture20110811.binetflow'),\n",
        "        (3, 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-44/detailed-bidirectional-flow-labels/capture20110812.binetflow'),\n",
        "        (4, 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-45/detailed-bidirectional-flow-labels/capture20110815.binetflow'),\n",
        "        (5, 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-46/detailed-bidirectional-flow-labels/capture20110815-2.binetflow'),\n",
        "        (6, 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-47/detailed-bidirectional-flow-labels/capture20110816.binetflow'),\n",
        "        (7, 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-48/detailed-bidirectional-flow-labels/capture20110816-2.binetflow'),\n",
        "        (8, 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-49/detailed-bidirectional-flow-labels/capture20110816-3.binetflow'),\n",
        "        (9, 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-50/detailed-bidirectional-flow-labels/capture20110817.binetflow'),\n",
        "        (10, 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-51/detailed-bidirectional-flow-labels/capture20110818.binetflow'),\n",
        "        (11, 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-52/detailed-bidirectional-flow-labels/capture20110818-2.binetflow'),\n",
        "        (12, 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-53/detailed-bidirectional-flow-labels/capture20110819.binetflow'),\n",
        "        (13, 'https://mcfp.felk.cvut.cz/publicDatasets/CTU-Malware-Capture-Botnet-54/detailed-bidirectional-flow-labels/capture20110815-3.binetflow')\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nStarting download for {len(datasets)} datasets...\")\n",
        "    for idx, url in datasets:\n",
        "        filename = url.split('/')[-1]\n",
        "        destination = os.path.join(main_dir, str(idx), filename)\n",
        "        folder_path = os.path.join(main_dir, str(idx))\n",
        "\n",
        "        print(f\"\\n[{idx}/13] Dataset {idx}:\")\n",
        "        if check_csv_in_folder(folder_path):\n",
        "            print(f\"  [SKIP] CSV already exists.\")\n",
        "            continue\n",
        "        download_file(url, destination)\n",
        "    print(\"\\nDownload complete!\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # 2. CONVERT BINETFLOW TO CSV\n",
        "    # =============================================================================\n",
        "    listDir = os.listdir(main_dir)\n",
        "    listCSV = []\n",
        "    print(\"Checking/Converting files...\")\n",
        "\n",
        "    for subDir in sorted(listDir, key=lambda x: int(x) if x.isdigit() else 0):\n",
        "        path_subDir = os.path.join(main_dir, subDir)\n",
        "        if not os.path.isdir(path_subDir): continue\n",
        "\n",
        "        # Check if CSV exists\n",
        "        csv_files = [f for f in os.listdir(path_subDir) if f.endswith('.csv')]\n",
        "        if csv_files:\n",
        "            listCSV.append(os.path.join(path_subDir, csv_files[0]))\n",
        "            continue\n",
        "\n",
        "        # If no CSV, look for binetflow to rename\n",
        "        binetflow_files = [f for f in os.listdir(path_subDir) if 'binetflow' in f]\n",
        "        if binetflow_files:\n",
        "            binetflow_file = os.path.join(path_subDir, binetflow_files[0])\n",
        "            new_name = subDir + '.csv'\n",
        "            rename(binetflow_file, new_name)\n",
        "            listCSV.append(os.path.join(path_subDir, new_name))\n",
        "            print(f\"  Converted {binetflow_files[0]} -> {new_name}\")\n",
        "\n",
        "    print(f\"\\nFound {len(listCSV)} CSV files:\")\n",
        "    for csv in listCSV:\n",
        "        print(f\"  {csv}\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # 3. IDENTIFY SCENARIOS\n",
        "    # =============================================================================\n",
        "    train_csvs = get_csv_paths(main_dir, TRAIN_SCENARIOS)\n",
        "    test_csvs = get_csv_paths(main_dir, TEST_SCENARIOS)\n",
        "    \n",
        "    print(f\"Found {len(train_csvs)} training files.\")\n",
        "    print(f\"Found {len(test_csvs)} testing files.\")\n",
        "\n",
        "    if not train_csvs:\n",
        "        print(\"No training files found! Exiting.\")\n",
        "        return\n",
        "\n",
        "    # =============================================================================\n",
        "    # 4. PRE-COMPUTE GLOBAL STATISTICS\n",
        "    # =============================================================================\n",
        "    print(\"=\"*70)\n",
        "    print(\"PRE-COMPUTING GLOBAL STATISTICS\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    target_csvs = train_csvs \n",
        "\n",
        "    # --- Step 1: Calculate Global Frequencies ---\n",
        "    print(\"\\n[1/4] Calculating global IP/Port frequencies...\")\n",
        "    freq_dicts = calculate_global_frequencies(target_csvs)\n",
        "\n",
        "    # --- Step 2: Detect Column Schema ---\n",
        "    print(\"\\n[2/3] Detecting column schema...\")\n",
        "    expected_columns = None\n",
        "    cols_samples = []\n",
        "\n",
        "    for csv_file in target_csvs[:5]:\n",
        "        try:\n",
        "            chunk = pd.read_csv(csv_file, nrows=5000, low_memory=False)\n",
        "            X_s, y_s, cols_s = process_batch_fast_v2(chunk, freq_dicts, expected_columns=None)\n",
        "            if cols_s:\n",
        "                cols_samples.extend(cols_s)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if cols_samples:\n",
        "        expected_columns = list(dict.fromkeys(cols_samples))  # Preserve order, remove duplicates\n",
        "        print(f\"  Detected {len(expected_columns)} feature columns\")\n",
        "    else:\n",
        "        print(\"  WARNING: Could not detect column schema!\")\n",
        "        return\n",
        "\n",
        "    # --- Step 3: Chu\u1ea9n b\u1ecb global_stats ---\n",
        "    n_features = len(expected_columns)\n",
        "\n",
        "    global_stats = {\n",
        "        'freq_dicts': freq_dicts,\n",
        "        'expected_columns': expected_columns,\n",
        "        'n_features': n_features\n",
        "    }\n",
        "\n",
        "    # =============================================================================\n",
        "    # 5. LOAD DATA INTO RAM\n",
        "    # =============================================================================\n",
        "    scaler = RobustScaler()\n",
        "    \n",
        "    print(\"\\nLoading TRAINING Data...\")\n",
        "    X_train, y_train = load_data_from_csvs(train_csvs, global_stats, desc=\"Train Data\", is_train=True, scaler=scaler)\n",
        "\n",
        "    # Optionally use only a part of the training set while keeping class ratio\n",
        "    if ispart:\n",
        "        subset_fraction = 0.1  # d\u00f9ng 10% d\u1eef li\u1ec7u train, v\u1eabn gi\u1eef \u0111\u00fang t\u1ec9 l\u1ec7 class\n",
        "        print(f\"\\n[ISPART] Using only {subset_fraction*100:.1f}% of training data with stratified sampling...\")\n",
        "        X_train, _, y_train, _ = train_test_split(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            train_size=subset_fraction,\n",
        "            stratify=y_train,\n",
        "            random_state=42\n",
        "        )\n",
        "        print(f\"New Train shape after ISPART: {X_train.shape}\")\n",
        "    \n",
        "    # Save statistics and scaler after processing training data\n",
        "    print(\"\\nSaving Global Statistics and Scaler...\")\n",
        "    save_global_stats(global_stats)\n",
        "    save_scaler(scaler)\n",
        "\n",
        "    print(\"\\nLoading TESTING Data...\")\n",
        "    X_test, y_test = load_data_from_csvs(test_csvs, global_stats, desc=\"Test Data\", is_train=False, scaler=scaler)\n",
        "\n",
        "    print(f\"\\nTrain shape: {X_train.shape}\")\n",
        "    print(f\"Test shape:  {X_test.shape}\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # 6. APPLY SMOTE (CH\u1ec8 TR\u00caN TRAIN SET)\n",
        "    # =============================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"APPLYING SMOTE TO BALANCE TRAINING DATA\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    print(f\"\\nPh\u00e2n ph\u1ed1i class TR\u01af\u1edaC SMOTE: {Counter(y_train)}\")\n",
        "    \n",
        "    # \u00c1p d\u1ee5ng SMOTE \u0111\u1ec3 t\u0103ng s\u1ed1 l\u01b0\u1ee3ng m\u1eabu Botnet\n",
        "    # CH\u1ec8 \u00e1p d\u1ee5ng tr\u00ean t\u1eadp train, KH\u00d4NG \u0111\u1ee5ng v\u00e0o test/val\n",
        "    smote = SMOTE(random_state=42, k_neighbors=5, sampling_strategy='auto')\n",
        "    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "    \n",
        "    print(f\"Ph\u00e2n ph\u1ed1i class SAU SMOTE: {Counter(y_train_res)}\")\n",
        "    print(f\"Train shape sau SMOTE: {X_train_res.shape}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # =============================================================================\n",
        "    # 7. MODEL SET UP\n",
        "    # =============================================================================\n",
        "    \n",
        "    # Validation Split (sau khi \u0111\u00e3 \u00e1p d\u1ee5ng SMOTE)\n",
        "    X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
        "        X_train_res, y_train_res, test_size=0.2, random_state=42, stratify=y_train_res\n",
        "    )\n",
        "\n",
        "    train_ds = FastBotnetDataset(X_train_final, y_train_final)\n",
        "    \n",
        "    print(\"\\nS\u1eed d\u1ee5ng CrossEntropyLoss (kh\u00f4ng d\u00f9ng class_weight v\u00ec \u0111\u00e3 c\u00f3 SMOTE)\")\n",
        "   \n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=0 # Avoid multiprocessing issues in some envs\n",
        "    )\n",
        "\n",
        "    valid_ds = FastBotnetDataset(X_val, y_val)\n",
        "    valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "    test_ds = FastBotnetDataset(X_test, y_test)\n",
        "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "    # Model\n",
        "    print(\"Initializing 2D CNN Image Model (pretrained)...\")\n",
        "    model = BotnetClassifier(base_model=None, n_features=n_features, n_classes=len(CLASS_TO_IDX))\n",
        "    model = model.to(device)\n",
        "\n",
        "    if summary:\n",
        "        # Input: \u1ea3nh 1x32x32\n",
        "        summary(model, input_size=(BATCH_SIZE, 1, IMAGE_SIZE, IMAGE_SIZE))\n",
        "    else:\n",
        "        print(model)\n",
        "\n",
        "    # B\u1ecf class_weight v\u00ec \u0111\u00e3 d\u00f9ng SMOTE \u0111\u1ec3 c\u00e2n b\u1eb1ng d\u1eef li\u1ec7u\n",
        "    # SMOTE \u0111\u00e3 t\u1ea1o ra d\u1eef li\u1ec7u th\u1eadt (synthetic) n\u00ean kh\u00f4ng c\u1ea7n \u00e9p model b\u1eb1ng tr\u1ecdng s\u1ed1\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "\n",
        "    # =============================================================================\n",
        "    # 8. TRAINING LOOP\n",
        "    # =============================================================================\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    print(f\"Starting training on {device}...\")\n",
        "    \n",
        "    for epoch in range(1, N_EPOCHS + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        \n",
        "    # Training\n",
        "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Ep {epoch}/{N_EPOCHS} [Train]\", leave=False):\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item() * X_batch.size(0)\n",
        "            \n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in tqdm(valid_loader, desc=f\"Ep {epoch}/{N_EPOCHS} [Val]\", leave=False):\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                val_loss += loss.item() * X_batch.size(0)\n",
        "                \n",
        "        epoch_val_loss = val_loss / len(valid_loader.dataset)\n",
        "        valid_losses.append(epoch_val_loss)\n",
        "        \n",
        "        print(f\"Epoch {epoch}: Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f}\")\n",
        "        \n",
        "        # Checkpoint\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            print(f\"  Val loss decreased ({best_val_loss:.4f} -> {epoch_val_loss:.4f}). Saving model...\")\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            best_val_loss = epoch_val_loss\n",
        "        plot_and_save_loss(train_losses, valid_losses, f'training_history_loss_{N_EPOCHS}.png')\n",
        "    # =============================================================================\n",
        "    # 9. RESULTS\n",
        "    # =============================================================================\n",
        "    print(\"Training Complete.\")\n",
        "\n",
        "# if __name__ == \"__main__\": # Block disabled for notebook import\n",
        "#     train_main(ispart=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CONTENT FROM: evaluate.py\n",
        "# ==========================================\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "import gc\n",
        "\n",
        "# from config import (  # Commented out for notebook compatibility\n",
        "    WORKING_DIR, BATCH_SIZE, TRAIN_SCENARIOS, TEST_SCENARIOS,\n",
        "    CLASS_TO_IDX, IDX_TO_CLASS, device\n",
        ")\n",
        "# from utils import get_csv_paths, create_directory, check_csv_in_folder, download_file, rename  # Commented out for notebook compatibility\n",
        "# from preprocessing_utils import (  # Commented out for notebook compatibility\n",
        "    quick_classify, calculate_global_frequencies, process_batch_fast_v2,\n",
        "    save_global_stats, load_global_stats, save_scaler, load_scaler\n",
        ")\n",
        "# from model import BotnetClassifier  # Commented out for notebook compatibility\n",
        "# from data_loader import FastBotnetDataset, load_data_from_csvs  # Commented out for notebook compatibility\n",
        "\n",
        "def compute_stats_from_train(main_dir):\n",
        "    \"\"\"\n",
        "    Recomputes global stats and scaler from training scenarios.\n",
        "    \"\"\"\n",
        "    print(\"Recomputing statistics from training data...\")\n",
        "    train_csvs = get_csv_paths(main_dir, TRAIN_SCENARIOS)\n",
        "\n",
        "    if not train_csvs:\n",
        "        print(\"No training files found! Please check TRAIN_SCENARIOS and dataset.\")\n",
        "        return None, None\n",
        "\n",
        "    # --- Step 1: Calculate Global Frequencies ---\n",
        "    print(\"\\n[1/2] Calculating global IP/Port frequencies...\")\n",
        "    freq_dicts = calculate_global_frequencies(train_csvs)\n",
        "\n",
        "    # --- Step 2: Detect Column Schema ---\n",
        "    print(\"\\n[2/2] Detecting column schema...\")\n",
        "    expected_columns = None\n",
        "    cols_samples = []\n",
        "\n",
        "    for csv_file in train_csvs[:5]:\n",
        "        try:\n",
        "            chunk = pd.read_csv(csv_file, nrows=5000, low_memory=False)\n",
        "            X_s, y_s, cols_s = process_batch_fast_v2(chunk, freq_dicts, expected_columns=None)\n",
        "            if cols_s:\n",
        "                cols_samples.extend(cols_s)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if cols_samples:\n",
        "        expected_columns = list(dict.fromkeys(cols_samples))\n",
        "        print(f\"  Detected {len(expected_columns)} feature columns\")\n",
        "    else:\n",
        "        print(\"  WARNING: Could not detect column schema!\")\n",
        "        return None, None\n",
        "\n",
        "    global_stats = {\n",
        "        'freq_dicts': freq_dicts,\n",
        "        'expected_columns': expected_columns,\n",
        "        'n_features': len(expected_columns)\n",
        "    }\n",
        "\n",
        "    # --- Step 4: Fit Scaler ---\n",
        "    print(\"\\nFitting Scaler on Training Data...\")\n",
        "    scaler = RobustScaler()\n",
        "    # We need to load training data to fit scaler. \n",
        "    # This might be heavy, but necessary for correct evaluation if stats are missing.\n",
        "    X_train, _ = load_data_from_csvs(train_csvs, global_stats, desc=\"Fitting Scaler\", is_train=True, scaler=scaler)\n",
        "    \n",
        "    # Save for future\n",
        "    save_global_stats(global_stats)\n",
        "    save_scaler(scaler)\n",
        "    \n",
        "    # Clean up RAM\n",
        "    del X_train\n",
        "    gc.collect()\n",
        "\n",
        "    return global_stats, scaler\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            \n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y_batch.cpu().numpy())\n",
        "            \n",
        "    return np.array(all_labels), np.array(all_preds)\n",
        "\n",
        "def plot_confusion_matrix_vietnamese(cm, classes, save_path):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    \n",
        "    # Map class names to Vietnamese if desired, or keep English but title in VN\n",
        "    # Common mappings: Normal -> B\u00ecnh th\u01b0\u1eddng, Botnet -> Botnet, C&C -> C&C\n",
        "    # Let's keep class names technical but title/labels in VN.\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.title('Ma tr\u1eadn nh\u1ea7m l\u1eabn (Confusion Matrix)', fontsize=16)\n",
        "    plt.ylabel('Nh\u00e3n th\u1ef1c t\u1ebf (True Label)', fontsize=12)\n",
        "    plt.xlabel('Nh\u00e3n d\u1ef1 \u0111o\u00e1n (Predicted Label)', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "def evaluate_main():\n",
        "    parser = argparse.ArgumentParser(description=\"Evaluate Botnet Detection Model\")\n",
        "    parser.add_argument('--recompute-stats', action='store_true', help=\"Recompute global stats and scaler from training data instead of loading.\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"Device: {device}\")\n",
        "    \n",
        "    # Check/Download Data (Reuse logic if needed, assuming data exists in CTU-13-Dataset)\n",
        "    main_dir = './CTU-13-Dataset/'\n",
        "    if not os.path.exists(main_dir):\n",
        "        print(\"Dataset directory not found. Please run train.py to download data first.\")\n",
        "        return\n",
        "\n",
        "    # 1. Load Stats and Scaler\n",
        "    if args.recompute_stats or not os.path.exists('global_stats.pkl') or not os.path.exists('scaler.pkl'):\n",
        "        global_stats, scaler = compute_stats_from_train(main_dir)\n",
        "        if global_stats is None:\n",
        "            return\n",
        "    else:\n",
        "        print(\"Loading existing statistics and scaler...\")\n",
        "        global_stats = load_global_stats()\n",
        "        scaler = load_scaler()\n",
        "    \n",
        "    if global_stats is None:\n",
        "        print(\"Failed to obtain global statistics.\")\n",
        "        return\n",
        "\n",
        "    # 2. Load Test Data\n",
        "    test_csvs = get_csv_paths(main_dir, TEST_SCENARIOS)\n",
        "    print(f\"Found {len(test_csvs)} testing files for evaluation.\")\n",
        "    \n",
        "    # Load data\n",
        "    X_test, y_test = load_data_from_csvs(test_csvs, global_stats, desc=\"Loading Test Data\", is_train=False, scaler=scaler)\n",
        "\n",
        "    print(f\"Test Data Shape: {X_test.shape}\")\n",
        "\n",
        "    # 3. Load Model\n",
        "    model_path = 'best_model.pth'\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Model file {model_path} not found!\")\n",
        "        return\n",
        "        \n",
        "    # n_features \u0111\u01b0\u1ee3c gi\u1eef cho t\u01b0\u01a1ng th\u00edch API nh\u01b0ng kh\u00f4ng d\u00f9ng trong CNN \u1ea3nh\n",
        "    model = BotnetClassifier(base_model=None, n_features=global_stats['n_features'], n_classes=len(CLASS_TO_IDX))\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model = model.to(device)\n",
        "    print(\"Model loaded successfully.\")\n",
        "\n",
        "    # 4. Evaluate\n",
        "    test_ds = FastBotnetDataset(X_test, y_test)\n",
        "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0) # 0 workers for safety\n",
        "    \n",
        "    y_true, y_pred = evaluate_model(model, test_loader, device)\n",
        "\n",
        "    # Quick sanity check: \u0111\u1ebfm s\u1ed1 m\u1eabu theo l\u1edbp trong ground-truth v\u00e0 d\u1ef1 \u0111o\u00e1n\n",
        "    label_indices = list(range(len(CLASS_TO_IDX)))\n",
        "    true_counts = {IDX_TO_CLASS[i]: int((y_true == i).sum()) for i in label_indices}\n",
        "    pred_counts = {IDX_TO_CLASS[i]: int((y_pred == i).sum()) for i in label_indices}\n",
        "    print(\"\\nPh\u00e2n b\u1ed1 nh\u00e3n (ground-truth):\", true_counts)\n",
        "    print(\"Ph\u00e2n b\u1ed1 nh\u00e3n (d\u1ef1 \u0111o\u00e1n):     \", pred_counts)\n",
        "\n",
        "    # 5. Metrics & Reporting (Vietnamese)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    \n",
        "    # Use a fixed label order based on CLASS_TO_IDX so that\n",
        "    # confusion matrix and classification report are consistent\n",
        "    label_indices = list(range(len(CLASS_TO_IDX)))\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=label_indices)\n",
        "    \n",
        "    # Prepare output directory\n",
        "    metrics_dir = 'metrics'\n",
        "    create_directory(metrics_dir)\n",
        "    \n",
        "    report_path = os.path.join(metrics_dir, 'evaluation_report.txt')\n",
        "    cm_path = os.path.join(metrics_dir, 'confusion_matrix.png')\n",
        "    \n",
        "    report_content = [\n",
        "        \"B\u00c1O C\u00c1O \u0110\u00c1NH GI\u00c1 M\u00d4 H\u00ccNH (MODEL EVALUATION REPORT)\",\n",
        "        \"=\"*50,\n",
        "        f\"K\u1ecbch b\u1ea3n ki\u1ec3m th\u1eed (Test Scenarios): {TEST_SCENARIOS}\",\n",
        "        f\"T\u1ed5ng s\u1ed1 m\u1eabu (Total Samples): {len(y_true)}\",\n",
        "        \"-\"*50,\n",
        "        f\"\u0110\u1ed9 ch\u00ednh x\u00e1c (Accuracy): {accuracy:.4f}\",\n",
        "        f\"\u0110\u1ed9 ch\u00ednh x\u00e1c (Precision - Weighted): {precision:.4f}\",\n",
        "        f\"\u0110\u1ed9 nh\u1ea1y (Recall - Weighted): {recall:.4f}\",\n",
        "        f\"\u0110i\u1ec3m F1 (F1-Score - Weighted): {f1:.4f}\",\n",
        "        \"=\"*50,\n",
        "        \"Chi ti\u1ebft theo l\u1edbp (Class-wise Details):\"\n",
        "    ]\n",
        "    \n",
        "    # Class-wise metrics\n",
        "    # Re-map indices to class names (must match label_indices order)\n",
        "    classes = [IDX_TO_CLASS[i] for i in label_indices]\n",
        "    \n",
        "    # Calculate per-class precision/recall/f1.\n",
        "    # Explicitly pass labels so that number of labels matches target_names,\n",
        "    # even if some classes do not appear in y_true/y_pred.\n",
        "    from sklearn.metrics import classification_report\n",
        "    cls_report = classification_report(\n",
        "        y_true,\n",
        "        y_pred,\n",
        "        labels=label_indices,\n",
        "        target_names=classes,\n",
        "        output_dict=True,\n",
        "        zero_division=0,\n",
        "    )\n",
        "    \n",
        "    for cls in classes:\n",
        "        metrics = cls_report.get(cls, {})\n",
        "        report_content.append(f\"\\nL\u1edbp: {cls}\")\n",
        "        report_content.append(f\"  Precision: {metrics.get('precision', 0):.4f}\")\n",
        "        report_content.append(f\"  Recall:    {metrics.get('recall', 0):.4f}\")\n",
        "        report_content.append(f\"  F1-Score:  {metrics.get('f1-score', 0):.4f}\")\n",
        "        \n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(report_content))\n",
        "        \n",
        "    print(f\"\\nReport saved to {report_path}\")\n",
        "    print('\\n'.join(report_content))\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    plot_confusion_matrix_vietnamese(cm, classes, cm_path)\n",
        "    print(f\"Confusion Matrix saved to {cm_path}\")\n",
        "\n",
        "# if __name__ == \"__main__\": # Block disabled for notebook import\n",
        "#     evaluate_main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# EXECUTION: TRAIN\n",
        "# ==========================================\n",
        "if __name__ == '__main__':\n",
        "    print('Starting Training...')\n",
        "    # Set ispart=False for full training\n",
        "    train_main(ispart=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# EXECUTION: EVALUATE\n",
        "# ==========================================\n",
        "import sys\n",
        "# Simulate command line arguments. Use empty list for defaults.\n",
        "# To recompute stats: sys.argv = ['evaluate.py', '--recompute-stats']\n",
        "sys.argv = ['evaluate.py'] \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('Starting Evaluation...')\n",
        "    evaluate_main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# EXECUTION: ANALYZE CSV (Example)\n",
        "# ==========================================\n",
        "# sys.argv = ['analyze_csv.py', '--file', './CTU-13-Dataset/1/capture20110810.csv']\n",
        "# analyze_csv_main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}