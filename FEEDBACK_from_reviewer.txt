Here is a step-by-step guide to revising the paper, categorized by urgency.
Phase 1: The "Fatal Flaws" (Must Fix to Avoid Rejection)
These are the issues that make the paper scientifically invalid. You must address these before rewriting the text.
1. Fix the IP/Port Encoding (Reviewer 2, Major #1)
The Problem: You converted IP addresses (e.g., 192.168.1.1) into single numbers (Label Encoding). This teaches the Neural Network that IP #5000 is "greater than" IP #100. This is mathematically false and biases the model.
The Fix: You must change how you represent IPs/Ports.
Option A: Remove IPs entirely if they bias the model toward specific distinct attacks.
Option B (Better): Use Binary Encoding or Frequency Encoding.
Option C (Best for CNNs): Split the IP into 4 octets (4 separate pixels/features) rather than one massive integer.
2. Address "Artificial Sparsity" & Dimensionality (Reviewer 2, Major #3 & #7)
The Problem: You took ~85 features and padded them with zeros to make a 200x200 image (40,000 pixels). That means 99.8% of your data is zeros. Reviewer 2 is right; this is inefficient.
The Fix: You need to justify the "Traffic-to-Image" mapping.
Don't just fill a grid with zeros. Consider using a Hilbert Curve or a meaningful matrix arrangement where related features are adjacent.
If you must use zero-padding, admit it as a limitation in the Discussion and explain why CNNs are still useful here (e.g., "to exploit spatial correlations between adjacent packet features").
3. Fix the Sampling Strategy (Reviewer 2, Major #2)
The Problem: You threw away >99% of the benign traffic. Your model is learning from a "toy" dataset, not reality.
The Fix:
Do not randomly discard 99% of the majority class.
Use SMOTE (Synthetic Minority Over-sampling Technique) to increase the botnet samples, or use a weighted loss function in the CNN to handle class imbalance.
You must train on a larger, more representative slice of the background traffic.

Phase 2: Strengthening the Experiments (Required for Credibility)
1. Reveal the "Black Box" (Reviewer 1 & 2)
Action: You must add a specific section titled "CNN Architecture Specification."
Include: A table or diagram showing:
Number of Convolutional Layers.
Filter sizes (e.g., 3x3).
Number of filters per layer (e.g., 32, 64).
Activation functions (e.g., ReLU, Softmax).
Pooling layers (Max/Average).
Dropout rates (to prove you handled overfitting).
2. Expand Metrics (Reviewer 1)
Action: Stop relying on Accuracy.
Include: Precision, Recall, F1-Score, False Positive Rate (FPR), and False Negative Rate (FNR).
Crucial: Include the Confusion Matrix.
3. Compare with SOTA & Baselines (Reviewer 2, Major #4 & #5)
The Problem: 97.16% is low for CTU-13.
Action: You must compare your CNN against:
Standard ML: Random Forest and XGBoost (Run these experiments; they often beat deep learning on tabular data).
Existing Papers: Cite the papers achieving 99.9% (mentioned by Reviewer 2) and explain why your method is still interesting.
Defense: If your accuracy is lower, you must argue that your model is more robust, faster, or requires less manual feature engineering than the 99.9% models.
4. Expand Scope (Reviewer 2, Minor #3)
Action: Do not rely only on Scenario 8 ("Murlo"). Run the model on at least Scenario 10 (Rbot) and Scenario 42 (or similar) to prove the model works on different botnet types.

Phase 3: Writing & Narrative Revisions
1. Clarify Novelty (Reviewer 1)
The reviewers know "Traffic-to-Image" is old (2018).
Pivot your claim: Don't say "We invented traffic-to-images." Say: "We systematically analyze the impact of resolution scaling and feature spatiality on CNN performance, which has been overlooked in previous transfer-learning approaches."
2. Add a "Discussion" Section (Reviewer 2, Minor #1)
Add a section discussing the limitations. Acknowledge that tabular data does not have the same spatial properties as a photograph of a cat or dog. Discussing this limitation actually makes the paper stronger because it shows you understand the theory.
3. Fix Typos and References
Change "Awad et al., 2026" to the correct year.
Fix the Figure 3 / Figure 1 reference error.
 
Guide for the Response Letter (Rebuttal)
When you submit the revised version, you must write a response letter. Here is the strategy for the tough comments:
To Reviewer 1:
Tone: Grateful and precise.
Response: "We thank the reviewer for highlighting the need for comprehensive metrics. We have added Table X reporting F1, Precision, and Recall. We have also added Section Y detailing the CNN architecture layer-by-layer."
To Reviewer 2 (The Danger Zone):
Tone: Humble and scientifically rigorous. Do not argue; demonstrate you fixed it.
On Encoding: "We agree with the reviewer that Label Encoding introduced bias. We have re-processed the dataset using Frequency/Binary encoding to eliminate ordinal assumptions."
On Sparsity: "We acknowledge the sparsity issue. We have added a comparison with Random Forest (Table Z) to show trade-offs and added a Discussion paragraph on the 'Curse of Dimensionality' regarding our feature mapping."
On Accuracy: "While our accuracy (97%) is below the absolute peak of ensemble methods (99%), our goal was to evaluate the robustness of representation learning. We have added a comparison table to contextulize our results against SOTA."
Summary Checklist for Acceptance
Re-code Data: Remove Label Encoding for IPs.
Re-Train: Train on more data (less down-sampling) and at least 3 Scenarios.
Run Baselines: Run Random Forest/XGBoost on the same data for comparison.
Write Specs: Add the CNN architecture table.
Update Tables: Add Precision/Recall/F1/Confusion Matrix.
Tone Shift: Admit limitations in the Discussion section.
If you do these things, you move the paper from "Methodologically Flawed" to "Rigorous Comparative Study," which has a high chance of acceptance.

Here is a realistic assessment of your chances and the specific risks you still face.
The Good News
If you address the IP encoding and sampling issues (Reviewer 2's major complaints), you immediately move the paper from "scientifically invalid" to "valid candidate."
Reviewers usually appreciate when authors take "Major Revisions" seriously.
Reviewer 1â€™s issues (missing metrics, architecture details) are easy fixes.
Reviewer 2 wants to see rigor. If you show a table comparing your CNN to Random Forest and admit the limitations, you satisfy their need for scientific honesty.
The Remaining Risks (The "Traps")
Even after you revise, the paper could still be rejected for three specific reasons. You need to watch out for these:
Risk 1: The "New" Results are Worse
When you fix the Label Encoding (IP addresses), your model might lose accuracy.
Why? Your current model might be "cheating" by learning that "High IP Number = Malicious." When you remove that bias, the accuracy might drop from 97% to 92%.
Solution: If accuracy drops, do not hide it. Instead, lean into feature engineering. Use the "Discussion" section to explain why it dropped and what that reveals about the difficulty of the dataset. Honesty is better than suspicious perfection.
Risk 2: The SOTA Comparison (The 99.9% Problem)
Reviewer 2 pointed out that existing methods get 99.9% accuracy. Your CNN gets 97%.
The Trap: If you resubmit showing 97.5% and simply say "We detected botnets," Reviewer 2 will ask: "Why should we use your complex CNN when a simple Random Forest gets 99.9%?"
The Fix: You must change the narrative of the paper. Do not sell the paper on "High Accuracy." Sell it on:
Automation: "Unlike Random Forest which requires manual feature extraction, our CNN learns features automatically."
Robustness: "Our method is less reliant on specific IP addresses, making it better for detecting new botnets (Zero-day attacks), even if overall accuracy is slightly lower on known data."
Risk 3: Lack of Novelty (Reviewer 1)
Reviewer 1 correctly noted that "Traffic-to-Image" is old news (2018).
The Trap: If your revision still acts like converting traffic to images is a brand new idea, Reviewer 1 will reject it.
The Fix: Update the Introduction. Explicitly state: "While traffic-to-image conversion is established (Citations X, Y, Z), previous works have not adequately analyzed the impact of resolution upscaling vs. feature sparsity. This paper fills that gap."
How to Guarantee Acceptance (The "Kill Shot")
To lock in the acceptance, add one specific subsection to your Results: "Cross-Scenario Generalization."
Reviewer 2 mentioned you only used the "Murlo" scenario.
Train your model on "Murlo."
Test it on a completely different botnet scenario (e.g., "Neris" or "Rbot") without retraining.
If your CNN performs decent (even 85-90%) on the unseen botnet, you have a massive winning argument: "Traditional methods overfit to specific attacks (99% accuracy), but our CNN generalizes better to unknown attacks."
Summary
If you just "fix the typos" and add a confusion matrix, the paper will be rejected.
If you re-run the code to fix the IP bias, add the Random Forest comparison, and rewrite the introduction to claim better generalization rather than just raw accuracy, the paper has a very high probability (80%+) of acceptance.


